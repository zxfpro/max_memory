Metadata-Version: 2.4
Name: max-memory
Version: 0.1.2
Summary: Add your description here
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.116.1
Requires-Dist: llama-index>=0.13.0
Requires-Dist: llama-index-vector-stores-qdrant>=0.7.1
Requires-Dist: llmada>=1.0.3
Requires-Dist: pyvis>=0.3.2
Requires-Dist: qdrant-client>=1.15.1
Requires-Dist: toml==0.10.2
Requires-Dist: uvicorn>=0.35.0
Requires-Dist: volcengine-python-sdk>=4.0.10

<!--
 * @Author: 823042332@qq.com 823042332@qq.com
 * @Date: 2025-08-01 14:31:15
 * @LastEditors: 823042332@qq.com 823042332@qq.com
 * @LastEditTime: 2025-08-12 16:52:16
 * @FilePath: /max_memory/README.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
# 记忆系统

# TOOD 1 加统计类决定分数
## 概念检索
#TODO radio



## 1 什么结构?
主要分为事件和概念两个维度, 两个维度有彼此联系

事件以组合模式存在, 每一个事件都带有发生的概率, 而概率则基于事件历史发生的统计 (后续考虑条件概率)

概念则以有向图的形式存在, 以马克思哲学的对对关系进行组织, 具有解释具体内涵, 的作用, 概念间的链接, 还有概率, 代表深度为1时的加权平均归一化值,

## 如何检索?

检索分为两个步骤, 1使用向量检索, 任何事件或者概念都是检索的内容,

精细化检索, 检索的精度要求会很高, 90->

第一步检索完毕以后, 会根据对应的结构和比重, 寻找出事件的相关内容

## 如何更新?    就像水滴入海一样
批量更新和少量更新的原理是相同的

1 将文章或者交流转化为标准的数据结构格式

2 使用向量检索出距离最近的10条数据, 分数在95以上 (考虑北斗七星检索方式)

3 让大模型判断是否需要融合, 自然, 之前的存储之间也可以融合

    1 如果不相融和, 直接录入
    
    2 如果需要融合, 则将他们按照一定的算法进行融合, 删除库内对应的元数据, 而存储融合后的新数据 注意链接性
    
4 不断重复这个过程, 可以分批次, 让大模型来保证每一批中的彼此都不融合, 就可以进行这个操作

    1 但是多批之间又有不同, 第一批融合完毕录入的内容,很可能在第二批中仍被检出和修改


## 如何使用?
1 用户做出问题或者输入文章
2 大模型分解出文章中的事件和概念, 分别使用检索, 检索出对应的事件组和概念组 存放到内存中
3 将内存中的事件组和概念组同步到system prompt 中, 概念采用名词解释的方式, 而事件则采用树结构
4 回答用户的问题,
5 用户再做回应, 大模型根据回应, 会有如下反应
    1 无动作
    2 用户对关系做出了调整, 大模型调整对应的内存和system prompt
    3 用户聊天, 但没有提出新的概念, 或者更换话题, 那么大模型调整对应内存和sp
    4 用户提出的新的概念, 大模型针对新概念进行检索,会有以下两种情况
        1大模型检索出了内容, 那么就合并到内存中, 并更新sp
        2大模型没有检索出内容, 那么就过一个总结prompt 实时提取结构合并到内存中, 并更新到sp
        在session结束后, 内存更新的部分要入库
    5 如果session持续过长,或者sp 过长, 就要触发垃圾回收机制, 内存中会有以key 为单位的调用次数记录, 调用度低的就在内存中删除, 存放到暂存区
        1在session 结束后, 同内存中的内容一并进行更新操作
        2 如果用户触发了在暂存区的内容, 那么有限从暂存区中调取, 如果失败才去做向量,
    6 如果用户提到的内容, 存在与以调取部分的枝干, 则不进行向量检索, 如果用户的意愿强烈才检索, 叶子也遵循类似原则
6 计划对用户的记忆实行类似github的管理, 这样,我们就可以让记忆采用多分支的方式进行


## 如何训练对应的大模型

1 事件和概率的关联性, 可以考虑采用增量训练加小模型来做
2 大模型具备大量知识, 所以他们应该具有对应的潜力, 只需要使用对应的prompt 即可诱导出来

考虑有无必要性


AI Change History
from program_writing_assistant.core import EditCode
ec = EditCode('/Users/zhaoxuefeng/GitHub/max_memory/src/max_memory/core.py')
ec.edit('优化一下 show_graph 函数, 呈现方式改为显示名称')
ec.edit('可见, find_related_edges_greedy_flexible_networkx 函数已经验证了其正确性, 将它整合到 Graphs 类中, 增加合并能力,注意, 要要考虑(name2id id2entities)等 ')
ec.edit('merge_graphs_with_advanced_aliases 函数已经验证了其正确性, 将它整合到 Graphs 类中, 增加合并其他图的能力,注意, 要要考虑(name2id id2entities)等 ')
ec.edit('我发现, merge_other_graph函数中的 node_mapping参数, 需要传入 id 的字典对应表,我希望使用name来做对应 可以使用get_nodes_by_name函数来转换')
ec.edit('做一个DiGraphs 继承Graphs 变为 nx.DiGraph() 重写一下 show_graph 方法')
ec.edit("参照Entity_Graph 编写Event_Graph 类, 重写process 可以参照里面的event_process 函数")
ec.edit("重写Digraphs 中的 参考注释中的find_nodes_by_depth ")
ec.edit("编写 Memory 类, 其他的已经成型,不要改动, Memory 类的核心宗旨是提供一个聊天的对外接口, 允许聊天, 存储,检索等功能 ")
